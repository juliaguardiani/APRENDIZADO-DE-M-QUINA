{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Checkpoint2_comites(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BmUrHS6h946h",
        "outputId": "f5fc2d2c-f3b8-4c61-e47a-a8cba0fee3f9"
      },
      "source": [
        "import pandas as pd\n",
        "dados_preProcessado = pd.read_csv('https://raw.githubusercontent.com/diego2017003/knn_treeClassifier/main/dados_preProcessados.csv')\n",
        "dados_originais = pd.read_csv('https://raw.githubusercontent.com/diego2017003/knn_treeClassifier/main/heart.csv')\n",
        "dados_preProcessado.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "dados_originais.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trtbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalachh</th>\n",
              "      <th>exng</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slp</th>\n",
              "      <th>caa</th>\n",
              "      <th>thall</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trtbps  chol  fbs  ...  exng  oldpeak  slp  caa  thall  output\n",
              "0   63    1   3     145   233    1  ...     0      2.3    0    0      1       1\n",
              "1   37    1   2     130   250    0  ...     0      3.5    0    0      2       1\n",
              "2   41    0   1     130   204    0  ...     0      1.4    2    0      2       1\n",
              "3   56    1   1     120   236    0  ...     0      0.8    2    0      2       1\n",
              "4   57    0   0     120   354    0  ...     1      0.6    2    0      2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "XbsFmT7aelKu",
        "outputId": "7208e885-52ca-4b85-ee35-f281b353458f"
      },
      "source": [
        "dados_preProcessado.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>trtbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>thalachh</th>\n",
              "      <th>exng</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slp_0</th>\n",
              "      <th>slp_1</th>\n",
              "      <th>slp_2</th>\n",
              "      <th>thall_0</th>\n",
              "      <th>thall_1</th>\n",
              "      <th>thall_2</th>\n",
              "      <th>thall_3</th>\n",
              "      <th>caa_0</th>\n",
              "      <th>caa_1</th>\n",
              "      <th>caa_2</th>\n",
              "      <th>caa_3</th>\n",
              "      <th>caa_4</th>\n",
              "      <th>restecg_Normal</th>\n",
              "      <th>restecg_Possui_anormalidade</th>\n",
              "      <th>restecg_Provável_hipertrofia</th>\n",
              "      <th>cp_Angina_atipica</th>\n",
              "      <th>cp_Angina_tipica</th>\n",
              "      <th>cp_Assintomatico</th>\n",
              "      <th>cp_Não_Angina</th>\n",
              "      <th>restecg_Possui_anormalidades</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.952197</td>\n",
              "      <td>1</td>\n",
              "      <td>0.763956</td>\n",
              "      <td>-0.256334</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015443</td>\n",
              "      <td>0</td>\n",
              "      <td>1.087338</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.003306</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.915313</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>0.072199</td>\n",
              "      <td>0</td>\n",
              "      <td>1.633471</td>\n",
              "      <td>0</td>\n",
              "      <td>2.122573</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.996705</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.474158</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>-0.816773</td>\n",
              "      <td>0</td>\n",
              "      <td>0.977514</td>\n",
              "      <td>0</td>\n",
              "      <td>0.310912</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.003306</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.180175</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.663867</td>\n",
              "      <td>-0.198357</td>\n",
              "      <td>0</td>\n",
              "      <td>1.239897</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.206705</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.996705</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.290464</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.663867</td>\n",
              "      <td>2.082050</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583939</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.379244</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.996705</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age  sex    trtbps  ...  cp_Não_Angina  restecg_Possui_anormalidades  output\n",
              "0  0.952197    1  0.763956  ...              1                             0       1\n",
              "1 -1.915313    1 -0.092738  ...              0                             1       1\n",
              "2 -1.474158    0 -0.092738  ...              0                             0       1\n",
              "3  0.180175    1 -0.663867  ...              0                             1       1\n",
              "4  0.290464    0 -0.663867  ...              0                             1       1\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRNn9L9t-2ek"
      },
      "source": [
        "Este notebook terá foco na utilização de comitês para a base de dado que vem sendo trabalhada durante o projeto.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ro4KccXS2r"
      },
      "source": [
        "##1. **Boost**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf4ml5g4Y_cq"
      },
      "source": [
        "### 1.1 MLPClassifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkZg3f7c82Ho"
      },
      "source": [
        "#### Dados originais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTrUCuVwX1qz"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "class customMLPClassifer(MLPClassifier):\n",
        "    def resample_with_replacement(self, X_train, y_train, sample_weight):\n",
        "\n",
        "        # normalize sample_weights if not already\n",
        "        sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
        "\n",
        "        X_train_resampled = np.zeros((len(X_train), len(X_train[0])), dtype=np.float32)\n",
        "        y_train_resampled = np.zeros((len(y_train)), dtype=np.int)\n",
        "        for i in range(len(X_train)):\n",
        "            # draw a number from 0 to len(X_train)-1\n",
        "            draw = np.random.choice(np.arange(len(X_train)), p=sample_weight)\n",
        "\n",
        "            # place the X and y at the drawn number into the resampled X and y\n",
        "            X_train_resampled[i] = X_train[draw]\n",
        "            y_train_resampled[i] = y_train[draw]\n",
        "\n",
        "        return X_train_resampled, y_train_resampled\n",
        "\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        if sample_weight is not None:\n",
        "            X, y = self.resample_with_replacement(X, y, sample_weight)\n",
        "\n",
        "        return self._fit(X, y, incremental=(self.warm_start and\n",
        "                                            hasattr(self, \"classes_\")))\n",
        "\n",
        " \n",
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "mlpBoost_10 = AdaBoostClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "mlpBoost_15 = AdaBoostClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "mlpBoost_20 = AdaBoostClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7,algorithm='SAMME')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q1QbjTWfgCi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "968bf99b-048a-4036-9ca4-69b7c4fbbfdd"
      },
      "source": [
        "'''conda update anaconda-navigator\n",
        "anaconda-navigator --reset\n",
        "conda update anaconda-client\n",
        "conda update -f anaconda-client'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'conda update anaconda-navigator\\nanaconda-navigator --reset\\nconda update anaconda-client\\nconda update -f anaconda-client'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSTTfR_du9Sz",
        "outputId": "9fc5c599-2513-4eee-99ea-ea9535e31658"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import precision_score\n",
        "import math\n",
        "#scoring = ['precision_macro', 'recall_macro']\n",
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=mlpBoost_10, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=mlpBoost_15, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=mlpBoost_20, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn9ToSaw8XAR",
        "outputId": "ef6e28aa-a459-43b6-8e51-8d824fe086c6"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8302242296746941 std: 0.08148934320862557\n",
            "n = 15 Precisão média :0.8162749466735535 std: 0.08130395367961846\n",
            "n = 20 Precisão média :0.8042343604108311 std: 0.05669974628070068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhvjOcwS853A"
      },
      "source": [
        "#### Dados Processados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPPCqXZB89aa"
      },
      "source": [
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "mlpBoost_p_10 = AdaBoostClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "mlpBoost_p_15 = AdaBoostClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=15, random_state=7,algorithm='SAMME')\n",
        "mlpBoost_p_20 = AdaBoostClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=20, random_state=7,algorithm='SAMME')\n",
        "#mlpBoost_p_10 = AdaBoostClassifier(base_estimator = customMLPClassifer(activation='identity', hidden_layer_sizes=(12,), solver='adam'),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "#mlpBoost_p_15 = AdaBoostClassifier(base_estimator = customMLPClassifer(activation='identity', hidden_layer_sizes=(12,), solver='adam'),n_estimators=15, random_state=7,algorithm='SAMME')\n",
        "#mlpBoost_p_20 = AdaBoostClassifier(base_estimator = customMLPClassifer(activation='identity', hidden_layer_sizes=(12,), solver='adam'),n_estimators=20, random_state=7,algorithm='SAMME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9j3a3Wec7c4"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "nsu9lnRGc8vf",
        "outputId": "3424ea1f-deb0-4145-b35c-dcc8d38581b4"
      },
      "source": [
        "'''param_grid = [\n",
        "        {\n",
        "            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
        "            'solver' : ['lbfgs', 'sgd', 'adam'],\n",
        "            'hidden_layer_sizes': [\n",
        "             (1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,), (12,),(13,),(14,),(15,),(16,),(17,),(18,),(19,),(20,),(21,),(22,),(23,),(24,),\n",
        "             (25,),(26,),(27,),(28,),(29,),(30,),(31,),(32,),(33,),(34,),(35,)\n",
        "             ]\n",
        "        }\n",
        "       ]\n",
        "clf = GridSearchCV(MLPClassifier(), param_grid, cv=3,scoring='accuracy')\n",
        "\n",
        "clf.fit(X,y)\n",
        "\n",
        "\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print(clf.best_params_)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'param_grid = [\\n        {\\n            \\'activation\\' : [\\'identity\\', \\'logistic\\', \\'tanh\\', \\'relu\\'],\\n            \\'solver\\' : [\\'lbfgs\\', \\'sgd\\', \\'adam\\'],\\n            \\'hidden_layer_sizes\\': [\\n             (1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,), (12,),(13,),(14,),(15,),(16,),(17,),(18,),(19,),(20,),(21,),(22,),(23,),(24,),\\n             (25,),(26,),(27,),(28,),(29,),(30,),(31,),(32,),(33,),(34,),(35,)\\n             ]\\n        }\\n       ]\\nclf = GridSearchCV(MLPClassifier(), param_grid, cv=3,scoring=\\'accuracy\\')\\n\\nclf.fit(X,y)\\n\\n\\nprint(\"Best parameters set found on development set:\")\\nprint(clf.best_params_)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ijM8S0D9H4y"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=mlpBoost_p_10, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=mlpBoost_p_15, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=mlpBoost_p_20, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8mwhDj9cXs",
        "outputId": "bf010c31-f925-4945-8d57-daa3abc8ceba"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))\n",
        "#n = 10 Precisão média :0.852618703965453 std: 0.05008061533940598\n",
        "#n = 15 Precisão média :0.8335730682286411 std: 0.06789153517380564\n",
        "#n = 20 Precisão média :0.8373666868906808 std: 0.06344416567018743"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8384491514677272 std: 0.060535771479765105\n",
            "n = 15 Precisão média :0.8630253851325833 std: 0.06231037294910893\n",
            "n = 20 Precisão média :0.8608061114582013 std: 0.05086677557434708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8oQrUypYM3i"
      },
      "source": [
        "### 1.2 Naive Bayes\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Q-fruBYYNa"
      },
      "source": [
        "#### Dados originais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGmkkjkYQvH"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "nbBoost_10 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "nbBoost_15 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=15, random_state=7,algorithm='SAMME')\n",
        "nbBoost_20 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=20, random_state=7,algorithm='SAMME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96QXFXjIZBrH"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import precision_score\n",
        "import math\n",
        "#scoring = ['precision_macro', 'recall_macro']\n",
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=nbBoost_10, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=nbBoost_15, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=nbBoost_20, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5NlaFWZVnd",
        "outputId": "a88c4cd0-b561-40b1-b147-e8672c57a590"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8293866872291795 std: 0.07032541790351075\n",
            "n = 15 Precisão média :0.839855330428086 std: 0.07196711973407324\n",
            "n = 20 Precisão média :0.839855330428086 std: 0.07196711973407324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXj6i8-NZfrw"
      },
      "source": [
        "#### Dados Processados "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA1Uq09dZmIe"
      },
      "source": [
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "nbBoost_p_10 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=10, random_state=7,algorithm='SAMME')\n",
        "nbBoost_p_15 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=15, random_state=7,algorithm='SAMME')\n",
        "nbBoost_p_20 = AdaBoostClassifier(base_estimator = GaussianNB(),n_estimators=20, random_state=7,algorithm='SAMME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf4zZwARZ2UA"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=nbBoost_p_10, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=nbBoost_p_15, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=nbBoost_p_20, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDeB3h9yaCl9",
        "outputId": "fbe2e690-00e6-49bb-8745-550a7c2e58f6"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8040560408859474 std: 0.09210672977281531\n",
            "n = 15 Precisão média :0.8040560408859474 std: 0.09210672977281531\n",
            "n = 20 Precisão média :0.8040560408859474 std: 0.09210672977281531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeKXC_CSAWkH"
      },
      "source": [
        "## 2. Bagging\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq85DTrUAbTs"
      },
      "source": [
        "### 2.1 MLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZ_57PRAeu_"
      },
      "source": [
        "#### Dados originais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKF6tjD_Aic7"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "mlpBag_10 = BaggingClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7)\n",
        "mlpBag_15 = BaggingClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7)\n",
        "mlpBag_20 = BaggingClassifier(base_estimator = customMLPClassifer(),n_estimators=10, random_state=7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSB0RmjCpk7"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import precision_score\n",
        "import math\n",
        "#scoring = ['precision_macro', 'recall_macro']\n",
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=mlpBag_10, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=mlpBag_15, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=mlpBag_20, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGlXN3cGEwNX",
        "outputId": "2072291a-3113-4ab5-bb09-521ba1c1dda8"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8377009052608898 std: 0.06960447111959236\n",
            "n = 15 Precisão média :0.8420482744379804 std: 0.06135139587538859\n",
            "n = 20 Precisão média :0.8326179348257522 std: 0.07905223835649483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh2oTjlkIjPq"
      },
      "source": [
        "#### Dados PreProcessados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvPalwoKIsAo"
      },
      "source": [
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "mlpBag_p_10 = BaggingClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=10, random_state=7)\n",
        "mlpBag_p_15 = BaggingClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=15, random_state=7)\n",
        "mlpBag_p_20 = BaggingClassifier(base_estimator = customMLPClassifer(alpha=0.0001,learning_rate='constant',activation='identity', hidden_layer_sizes=(21,), solver='sgd'),n_estimators=20, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jEE9nlvJC29"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=mlpBag_p_10, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=mlpBag_p_15, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=mlpBag_p_20, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljITMC1hJWE4",
        "outputId": "6e5e0ee7-2d09-4423-f504-dfee8b947f8d"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8444788680102148 std: 0.05481679205089955\n",
            "n = 15 Precisão média :0.8437595782700271 std: 0.049509367139697884\n",
            "n = 20 Precisão média :0.843501372423586 std: 0.05807702840628681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoEi7rbOJvw-"
      },
      "source": [
        "### 2.2 Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT-W9XNgKFpA"
      },
      "source": [
        "#### Dados originais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeaAHiTyKgIX"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "nbBag_10 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=10, random_state=7)\n",
        "nbBag_15 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=15, random_state=7)\n",
        "nbBag_20 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=20, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvGRMeUNLMNw"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=nbBag_10, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=nbBag_15, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=nbBag_20, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNA1QK9JLGnc",
        "outputId": "aeb36c4c-f60d-4842-d18f-06b10901bad9"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8314622484069079 std: 0.059105151157382955\n",
            "n = 15 Precisão média :0.8184218206973626 std: 0.05917025228746715\n",
            "n = 20 Precisão média :0.8092972099280148 std: 0.05056133001621758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75yEalyMCCJ"
      },
      "source": [
        "#### Dados PreProcessados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kljD5ahMBlT"
      },
      "source": [
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "nbBag_p_10 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=10, random_state=7)\n",
        "nbBag_p_15 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=15, random_state=7)\n",
        "nbBag_p_20 = BaggingClassifier(base_estimator = GaussianNB(),n_estimators=20, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLd45oYCMpGg"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=nbBag_p_10, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=nbBag_p_15, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=nbBag_p_20, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLKXBemmMzfX",
        "outputId": "b0db5cc3-b3b7-4359-f067-60ee2012b6c6"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8034839180427416 std: 0.07957654234288679\n",
            "n = 15 Precisão média :0.8098564793050087 std: 0.07728274294416708\n",
            "n = 20 Precisão média :0.8169193878017408 std: 0.0742722387493331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT6O7thiz0F4"
      },
      "source": [
        "### 3. Stacking Homogêneo\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84vUpGON0qMl"
      },
      "source": [
        "#### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnQtixt0tTy"
      },
      "source": [
        "#### Dados originais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS_v73yhM_xl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "base_learners_10 = []\n",
        "base_learners_15 = []\n",
        "base_learners_20 = []\n",
        "for i in range(10):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('rf_'+str(i), MLPClassifier()))\n",
        "\n",
        "for i in range(15):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_15.append(('mlp_'+str(i), MLPClassifier()))\n",
        "\n",
        "for i in range(20):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_20.append(('mlp_'+str(i), MLPClassifier()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLSCl523zzPL"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "base_learners = [('rf_1', MLPClassifier())]\n",
        "\n",
        "mlpStk_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "mlpStk_15 = StackingClassifier(estimators = base_learners_15,cv=10)\n",
        "mlpStk_20 = StackingClassifier(estimators = base_learners_20,cv=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok9Xm7p34ry3"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=mlpStk_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=mlpStk_15, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=mlpStk_20, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAyaLyHZ65yh",
        "outputId": "b07c4ae7-6ea8-4618-c8bc-35e55cf38477"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8145623038636469 std: 0.05662330796770682\n",
            "n = 15 Precisão média :0.806052406450274 std: 0.046400642083073304\n",
            "n = 20 Precisão média :0.8099110302608755 std: 0.04683408043536126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg70QlsZUdWV"
      },
      "source": [
        "#### Dados PreProcessados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAnSwFRxUdBD"
      },
      "source": [
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "mlpStk_p_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "mlpStk_p_15 = StackingClassifier(estimators = base_learners_15,cv=10)\n",
        "mlpStk_p_20 = StackingClassifier(estimators = base_learners_20,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj6AREe1SEji"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=mlpStk_p_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=mlpStk_p_15, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=mlpStk_p_20, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K92WuPN8WTpk",
        "outputId": "5d1d98fa-4e5d-405d-b63a-a55b8d5c4472"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8102504978645134 std: 0.04952702398672045\n",
            "n = 15 Precisão média :0.7995353189838483 std: 0.03327987757151561\n",
            "n = 20 Precisão média :0.8055751391466434 std: 0.04219952646098446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc6apiH3fSwS"
      },
      "source": [
        "### 3.2 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjI1hN5xfdvq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "base_learners_10 = []\n",
        "base_learners_15 = []\n",
        "base_learners_20 = []\n",
        "for i in range(10):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('nb_'+str(i), GaussianNB()))\n",
        "\n",
        "for i in range(15):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_15.append(('nb_'+str(i), GaussianNB()))\n",
        "\n",
        "for i in range(20):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_20.append(('nb_'+str(i), GaussianNB()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl4tgOtmfi24"
      },
      "source": [
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "base_learners = [('rf_1', MLPClassifier())]\n",
        "\n",
        "nbStk_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "nbStk_15 = StackingClassifier(estimators = base_learners_15,cv=10)\n",
        "nbStk_20 = StackingClassifier(estimators = base_learners_20,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu4mtUNff9bM"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=nbStk_10, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_15 = cross_validate(estimator=nbStk_15, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_20 = cross_validate(estimator=nbStk_20, X= X, y= y, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5b9y75xgJD6",
        "outputId": "7f6a1a96-e39f-4ea9-d253-cddeb7727ab2"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_15 = np.array(scores_15['test_prec_macro']).mean()\n",
        "std_15 = np.array(scores_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_15)+ \" std: \"+str(std_15))\n",
        "mean_20 = np.array(scores_20['test_prec_macro']).mean()\n",
        "std_20 = np.array(scores_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_20)+ \" std: \"+str(std_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8092750470633596 std: 0.06355865857774357\n",
            "n = 15 Precisão média :0.8092750470633596 std: 0.06355865857774357\n",
            "n = 20 Precisão média :0.8092750470633596 std: 0.06355865857774357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzA47ZwBgXFl"
      },
      "source": [
        "#### Dados Preprocessados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUlJDGKFgaVb"
      },
      "source": [
        "nbStk_p_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "nbStk_p_15 = StackingClassifier(estimators = base_learners_15,cv=10)\n",
        "nbStk_p_20 = StackingClassifier(estimators = base_learners_20,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1mYz0z7gm9S"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_p_10 = cross_validate(estimator=nbStk_10, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_15 = cross_validate(estimator=nbStk_15, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)\n",
        "scores_p_20 = cross_validate(estimator=nbStk_20, X= X_p, y= y_p, scoring=scoring,cv=10, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHtlehlFg12z",
        "outputId": "b6591d31-1736-4a6a-a277-6a1ff80dad84"
      },
      "source": [
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n",
        "mean_p_15 = np.array(scores_p_15['test_prec_macro']).mean()\n",
        "std_p_15 = np.array(scores_p_15['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_15)+ \" std: \"+str(std_p_15))\n",
        "mean_p_20 = np.array(scores_p_20['test_prec_macro']).mean()\n",
        "std_p_20 = np.array(scores_p_20['test_prec_macro']).std()\n",
        "print(\"n = 20 Precisão média :\"+str(mean_p_20)+ \" std: \"+str(std_p_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8032499876714274 std: 0.07280909439688978\n",
            "n = 15 Precisão média :0.8032499876714274 std: 0.07280909439688978\n",
            "n = 20 Precisão média :0.8032499876714274 std: 0.07280909439688978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqxe33M7oOBp"
      },
      "source": [
        "### 4. Stacking heterogêneo  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5c4R3myoXxY"
      },
      "source": [
        "ad+mlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0SIIBSGoUpt"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "base_learners_10 = []\n",
        "\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('dt_'+str(i), DecisionTreeClassifier()))\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('mlp_'+str(i), MLPClassifier()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CmeuoV8ov_p"
      },
      "source": [
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "base_learners = [('rf_1', MLPClassifier())]\n",
        "\n",
        "mdStk_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "\n",
        "\n",
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "mdStk_p_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CsFyL54pK-Y"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=mdStk_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_10 = cross_validate(estimator=mdStk_p_10, X= X_p, y= y_p, scoring=scoring,cv=5, return_train_score=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ZSUtgPpc4w",
        "outputId": "1907710d-9b6e-4dbe-a762-73783df4d2b0"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 10 Precisão média :0.8522364252104964 std: 0.07203744448176708\n",
            "n = 15 Precisão média :0.8363776299380016 std: 0.05180095063579365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I25jsRSp7dE"
      },
      "source": [
        "mlp + nb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42IAkywyp_I_"
      },
      "source": [
        "base_learners_10 = []\n",
        "\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('dt_'+str(i), GaussianNB()))\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('mlp_'+str(i), MLPClassifier()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eGazhvbqDPo"
      },
      "source": [
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "base_learners = [('rf_1', MLPClassifier())]\n",
        "\n",
        "mnStk_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "\n",
        "\n",
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "mnStk_p_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyItNUlgqSkz",
        "outputId": "9bc55f99-91d3-4855-a9c5-bad48e4ec182"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=mnStk_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_10 = cross_validate(estimator=mnStk_p_10, X= X_p, y= y_p, scoring=scoring,cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JT_0q-pqXbo"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CxmNKkqcrU"
      },
      "source": [
        "nb + dt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XKoA5laqtnc"
      },
      "source": [
        "base_learners_10 = []\n",
        "\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('dt_'+str(i), GaussianNB()))\n",
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  base_learners_10.append(('mlp_'+str(i), DecisionTreeClassifier()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56FhDSAoqhmy"
      },
      "source": [
        "X = dados_originais.drop(columns=['output'])\n",
        "y = dados_originais.output\n",
        "base_learners = [('rf_1', MLPClassifier())]\n",
        "\n",
        "dnStk_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n",
        "\n",
        "\n",
        "X_p = dados_preProcessado.drop(columns=['output'])\n",
        "y_p = dados_preProcessado.output\n",
        "dnStk_p_10 = StackingClassifier(estimators = base_learners_10,cv=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02EXFIxSqfeM"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=dnStk_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_10 = cross_validate(estimator=dnStk_p_10, X= X_p, y= y_p, scoring=scoring,cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL8fNc21qeR-"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDbXADQq0hp"
      },
      "source": [
        "NB+MLP+DT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxnPLWXwq4m2"
      },
      "source": [
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores_10 = cross_validate(estimator=dnStk_10, X= X, y= y, scoring=scoring,cv=5, return_train_score=True)\n",
        "scores_p_10 = cross_validate(estimator=dnStk_p_10, X= X_p, y= y_p, scoring=scoring,cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlVPp6Mq24O"
      },
      "source": [
        "mean_10 = np.array(scores_10['test_prec_macro']).mean()\n",
        "std_10 = np.array(scores_10['test_prec_macro']).std()\n",
        "print(\"n = 10 Precisão média :\"+str(mean_10)+ \" std: \"+str(std_10))\n",
        "mean_p_10 = np.array(scores_p_10['test_prec_macro']).mean()\n",
        "std_p_10 = np.array(scores_p_10['test_prec_macro']).std()\n",
        "print(\"n = 15 Precisão média :\"+str(mean_p_10)+ \" std: \"+str(std_p_10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}